# NeoScan 后续开发建议

## 概述

本文档为NeoScan项目在开发阶段提供指导，确保当前的单Master开发环境能够平滑过渡到后续的Master集群架构，最大化代码复用，最小化重构成本。

## 1. 代码架构设计

### 1.1 服务模块化

#### 微服务架构
- 将Master节点的功能拆分为独立的服务模块
  - 任务调度服务（Task Scheduler）
  - 配置管理服务（Config Manager）
  - 数据处理服务（Data Processor）
  - API网关服务（API Gateway）
  - 用户认证服务（Auth Service）

#### 接口抽象
- 定义清晰的服务接口，便于后续服务拆分和独立部署
- 使用Go的interface定义服务契约
- 实现服务间的松耦合设计

```go
// 示例：任务调度服务接口
type TaskScheduler interface {
    ScheduleTask(task *Task) error
    CancelTask(taskID string) error
    GetTaskStatus(taskID string) (*TaskStatus, error)
    ListRunningTasks() ([]*Task, error)
}

// 示例：配置管理服务接口
type ConfigManager interface {
    GetConfig(key string) (interface{}, error)
    SetConfig(key string, value interface{}) error
    WatchConfig(key string) (<-chan interface{}, error)
}
```

#### 依赖注入
- 使用依赖注入模式，便于后续替换单例服务为分布式服务
- 推荐使用wire或dig等依赖注入框架
- 避免硬编码的服务依赖关系

### 1.2 状态管理

#### 无状态设计
- **核心原则**：Master服务尽量设计为无状态，所有状态信息存储在外部存储（数据库、缓存）
- **具体实现**：
  - 任务执行状态存储在MySQL数据库
  - Agent节点状态存储在Redis缓存
  - 扫描进度信息存储在数据库或消息队列
  - 系统配置信息存储在etcd或数据库

```go
// ❌ 错误的有状态设计
type TaskManager struct {
    runningTasks map[string]*Task  // 状态存储在内存中
    agentStatus  map[string]bool   // Agent状态存储在内存中
}

// ✅ 正确的无状态设计
type TaskManager struct {
    db    *sql.DB       // 数据库连接
    redis *redis.Client // Redis连接
}

func (tm *TaskManager) GetRunningTasks() ([]*Task, error) {
    // 从数据库查询运行中的任务
    return tm.db.Query("SELECT * FROM tasks WHERE status = 'running'")
}
```

#### 会话外部化
- **核心原则**：用户会话信息存储在Redis中，而不是内存中
- **具体实现**：
  - JWT Token存储在Redis
  - 用户权限信息缓存在Redis
  - 用户操作上下文存储在外部缓存

```go
// 会话管理示例
type SessionManager struct {
    redis *redis.Client
}

func (sm *SessionManager) StoreSession(sessionID string, user *User) error {
    sessionData, _ := json.Marshal(user)
    // 存储到Redis，设置过期时间
    return sm.redis.Set("session:"+sessionID, sessionData, 24*time.Hour).Err()
}

func (sm *SessionManager) GetSession(sessionID string) (*User, error) {
    // 从Redis获取会话信息
    data, err := sm.redis.Get("session:" + sessionID).Result()
    if err != nil {
        return nil, err
    }
    var user User
    json.Unmarshal([]byte(data), &user)
    return &user, nil
}
```

#### 避免本地缓存
- **核心原则**：减少本地内存缓存的使用，优先使用Redis等外部缓存
- **允许的本地缓存场景**：
  - 只读配置（系统启动时加载的不变配置）
  - 短期计算结果缓存（能容忍不一致）
  - 连接池（数据库连接池、Redis连接池等资源池）

```go
// 配置管理示例
type ConfigManager struct {
    redis *redis.Client
    db    *sql.DB
}

func (cm *ConfigManager) GetConfig(key string) (interface{}, error) {
    // 先从Redis获取
    if value, err := cm.redis.Get("config:" + key).Result(); err == nil {
        return value, nil
    }
    
    // Redis没有则从数据库获取并缓存到Redis
    value, err := cm.db.QueryRow("SELECT value FROM configs WHERE key = ?", key).Scan()
    if err != nil {
        return nil, err
    }
    
    // 缓存到Redis
    cm.redis.Set("config:"+key, value, time.Hour)
    return value, nil
}
```

## 2. 配置管理策略

### 2.1 配置外部化

#### 配置中心
- 从开发阶段就使用外部配置管理（如etcd、Consul或Kubernetes ConfigMap）
- 避免配置文件硬编码在代码中
- 支持配置的版本管理和回滚

#### 环境变量
- 关键配置通过环境变量注入，避免硬编码
- 使用配置结构体统一管理

```go
// 配置结构体示例
type Config struct {
    Database struct {
        Host     string `env:"DB_HOST" envDefault:"localhost"`
        Port     int    `env:"DB_PORT" envDefault:"3306"`
        Username string `env:"DB_USERNAME" envDefault:"root"`
        Password string `env:"DB_PASSWORD"`
    }
    Redis struct {
        Host     string `env:"REDIS_HOST" envDefault:"localhost"`
        Port     int    `env:"REDIS_PORT" envDefault:"6379"`
        Password string `env:"REDIS_PASSWORD"`
    }
    Server struct {
        Port int    `env:"SERVER_PORT" envDefault:"8080"`
        Mode string `env:"GIN_MODE" envDefault:"debug"`
    }
}
```

#### 动态配置
- 支持配置热更新，无需重启服务
- 实现配置变更的监听机制

```go
// 配置热更新示例
type ConfigWatcher struct {
    etcdClient *clientv3.Client
    callbacks  map[string][]func(string, interface{})
}

func (cw *ConfigWatcher) WatchConfig(key string, callback func(string, interface{})) {
    watchChan := cw.etcdClient.Watch(context.Background(), key)
    go func() {
        for watchResp := range watchChan {
            for _, event := range watchResp.Events {
                callback(string(event.Kv.Key), string(event.Kv.Value))
            }
        }
    }()
}
```

### 2.2 服务发现准备

#### 服务注册
- 即使在单Master环境下，也实现服务注册机制
- 为后续集群环境做准备

```go
// 服务注册示例
type ServiceRegistry struct {
    etcdClient *clientv3.Client
    serviceID  string
    ttl        int64
}

func (sr *ServiceRegistry) Register(serviceName, address string) error {
    lease, err := sr.etcdClient.Grant(context.Background(), sr.ttl)
    if err != nil {
        return err
    }
    
    key := fmt.Sprintf("/services/%s/%s", serviceName, sr.serviceID)
    _, err = sr.etcdClient.Put(context.Background(), key, address, clientv3.WithLease(lease.ID))
    if err != nil {
        return err
    }
    
    // 续租
    ch, kaerr := sr.etcdClient.KeepAlive(context.Background(), lease.ID)
    if kaerr != nil {
        return kaerr
    }
    
    go func() {
        for ka := range ch {
            // 处理续租响应
            _ = ka
        }
    }()
    
    return nil
}
```

#### 健康检查
- 实现标准的健康检查接口（如/health、/ready）
- 支持深度健康检查（数据库连接、Redis连接等）

```go
// 健康检查示例
type HealthChecker struct {
    db    *sql.DB
    redis *redis.Client
}

func (hc *HealthChecker) CheckHealth() map[string]interface{} {
    result := make(map[string]interface{})
    
    // 检查数据库连接
    if err := hc.db.Ping(); err != nil {
        result["database"] = map[string]interface{}{
            "status": "unhealthy",
            "error":  err.Error(),
        }
    } else {
        result["database"] = map[string]interface{}{
            "status": "healthy",
        }
    }
    
    // 检查Redis连接
    if err := hc.redis.Ping().Err(); err != nil {
        result["redis"] = map[string]interface{}{
            "status": "unhealthy",
            "error":  err.Error(),
        }
    } else {
        result["redis"] = map[string]interface{}{
            "status": "healthy",
        }
    }
    
    return result
}
```

#### 负载均衡准备
- API设计时考虑负载均衡的需求
- 确保API的幂等性

## 3. 数据存储设计

### 3.1 数据库设计

#### 读写分离准备
- 在代码层面区分读操作和写操作，使用不同的数据源
- 为后续数据库集群做准备

```go
// 数据库读写分离示例
type DatabaseManager struct {
    writeDB *sql.DB  // 主库连接
    readDB  *sql.DB  // 从库连接（开发阶段可以指向同一个数据库）
}

func (dm *DatabaseManager) GetWriteDB() *sql.DB {
    return dm.writeDB
}

func (dm *DatabaseManager) GetReadDB() *sql.DB {
    return dm.readDB
}

// 使用示例
func (ts *TaskService) CreateTask(task *Task) error {
    // 写操作使用主库
    db := ts.dbManager.GetWriteDB()
    _, err := db.Exec("INSERT INTO tasks (...) VALUES (...)", ...)
    return err
}

func (ts *TaskService) GetTask(taskID string) (*Task, error) {
    // 读操作使用从库
    db := ts.dbManager.GetReadDB()
    row := db.QueryRow("SELECT * FROM tasks WHERE id = ?", taskID)
    // ...
}
```

#### 事务边界
- 合理设计事务边界，避免跨服务的长事务
- 使用分布式事务模式（如Saga模式）

#### 数据分片考虑
- 大表设计时考虑后续分片的可能性
- 选择合适的分片键

```sql
-- 任务表设计示例（考虑分片）
CREATE TABLE tasks (
    id VARCHAR(64) PRIMARY KEY,  -- 使用UUID作为主键，便于分片
    user_id VARCHAR(64) NOT NULL, -- 可以作为分片键
    target_type VARCHAR(32) NOT NULL,
    target_value TEXT NOT NULL,
    status VARCHAR(32) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_user_id (user_id),
    INDEX idx_status (status),
    INDEX idx_created_at (created_at)
);
```

### 3.2 缓存策略

#### 分布式缓存
- 从开始就使用Redis等分布式缓存，而非本地缓存
- 设计合理的缓存键命名规范

```go
// 缓存键命名规范
const (
    CacheKeyUserSession = "session:%s"           // 用户会话
    CacheKeyAgentStatus = "agent:status:%s"      // Agent状态
    CacheKeyTaskResult  = "task:result:%s"       // 任务结果
    CacheKeyConfig      = "config:%s"            // 配置信息
)

// 缓存管理器
type CacheManager struct {
    redis *redis.Client
}

func (cm *CacheManager) Set(key string, value interface{}, expiration time.Duration) error {
    data, err := json.Marshal(value)
    if err != nil {
        return err
    }
    return cm.redis.Set(key, data, expiration).Err()
}

func (cm *CacheManager) Get(key string, dest interface{}) error {
    data, err := cm.redis.Get(key).Result()
    if err != nil {
        return err
    }
    return json.Unmarshal([]byte(data), dest)
}
```

#### 缓存一致性
- 设计缓存更新和失效策略，为集群环境做准备
- 实现缓存的主动失效机制

```go
// 缓存一致性示例
func (as *AgentService) UpdateAgentStatus(agentID string, status *AgentStatus) error {
    // 1. 更新数据库
    if err := as.updateAgentStatusInDB(agentID, status); err != nil {
        return err
    }
    
    // 2. 更新缓存
    cacheKey := fmt.Sprintf(CacheKeyAgentStatus, agentID)
    if err := as.cache.Set(cacheKey, status, time.Minute*10); err != nil {
        // 缓存更新失败，记录日志但不影响主流程
        log.Warnf("Failed to update cache for agent %s: %v", agentID, err)
    }
    
    // 3. 发布缓存失效事件（为集群环境准备）
    as.publishCacheInvalidation("agent_status", agentID)
    
    return nil
}
```

#### 缓存预热
- 系统启动时预加载常用数据
- 实现智能缓存预热策略

## 4. 通信协议设计

### 4.1 API设计

#### RESTful规范
- 严格遵循RESTful设计原则，便于负载均衡
- 使用标准的HTTP状态码
- 实现统一的错误响应格式

```go
// 统一响应格式
type APIResponse struct {
    Code    int         `json:"code"`
    Message string      `json:"message"`
    Data    interface{} `json:"data,omitempty"`
    TraceID string      `json:"trace_id,omitempty"`
}

// 错误响应
type ErrorResponse struct {
    Code    int    `json:"code"`
    Message string `json:"message"`
    Details string `json:"details,omitempty"`
    TraceID string `json:"trace_id"`
}
```

#### 幂等性
- 确保API操作的幂等性，支持重试机制
- 使用幂等键防止重复操作

```go
// 幂等性示例
func (tc *TaskController) CreateTask(c *gin.Context) {
    var req CreateTaskRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(400, ErrorResponse{Code: 400, Message: "Invalid request"})
        return
    }
    
    // 使用幂等键检查是否已经创建过
    idempotencyKey := c.GetHeader("Idempotency-Key")
    if idempotencyKey != "" {
        if existingTask, err := tc.taskService.GetTaskByIdempotencyKey(idempotencyKey); err == nil {
            c.JSON(200, APIResponse{Code: 200, Data: existingTask})
            return
        }
    }
    
    // 创建任务
    task, err := tc.taskService.CreateTask(&req, idempotencyKey)
    if err != nil {
        c.JSON(500, ErrorResponse{Code: 500, Message: "Failed to create task"})
        return
    }
    
    c.JSON(201, APIResponse{Code: 201, Data: task})
}
```

#### 版本控制
- 从开始就实现API版本控制
- 支持多版本并存

```go
// API版本控制示例
func SetupRoutes(r *gin.Engine) {
    v1 := r.Group("/api/v1")
    {
        v1.POST("/tasks", taskController.CreateTask)
        v1.GET("/tasks/:id", taskController.GetTask)
        v1.PUT("/tasks/:id", taskController.UpdateTask)
        v1.DELETE("/tasks/:id", taskController.DeleteTask)
    }
    
    v2 := r.Group("/api/v2")
    {
        v2.POST("/tasks", taskControllerV2.CreateTask)
        v2.GET("/tasks/:id", taskControllerV2.GetTask)
        // ...
    }
}
```

### 4.2 内部通信

#### gRPC标准化
- Master与Agent间的通信使用标准gRPC协议
- 定义清晰的protobuf接口

```protobuf
// agent.proto
syntax = "proto3";

package agent;

service AgentService {
    rpc RegisterAgent(RegisterAgentRequest) returns (RegisterAgentResponse);
    rpc HeartBeat(HeartBeatRequest) returns (HeartBeatResponse);
    rpc ExecuteTask(ExecuteTaskRequest) returns (stream ExecuteTaskResponse);
    rpc GetAgentStatus(GetAgentStatusRequest) returns (GetAgentStatusResponse);
}

message RegisterAgentRequest {
    string agent_id = 1;
    string agent_name = 2;
    string agent_version = 3;
    repeated string capabilities = 4;
    AgentResource resource = 5;
}

message AgentResource {
    int32 cpu_cores = 1;
    int64 memory_mb = 2;
    int64 disk_gb = 3;
}
```

#### 消息队列
- 异步任务处理使用消息队列（RabbitMQ），而非直接调用
- 实现消息的持久化和重试机制

```go
// 消息队列示例
type MessageQueue struct {
    conn    *amqp.Connection
    channel *amqp.Channel
}

func (mq *MessageQueue) PublishTask(task *Task) error {
    body, err := json.Marshal(task)
    if err != nil {
        return err
    }
    
    return mq.channel.Publish(
        "tasks",    // exchange
        "task.new", // routing key
        false,      // mandatory
        false,      // immediate
        amqp.Publishing{
            ContentType:  "application/json",
            Body:         body,
            DeliveryMode: amqp.Persistent, // 消息持久化
        },
    )
}

func (mq *MessageQueue) ConsumeTask(handler func(*Task) error) error {
    msgs, err := mq.channel.Consume(
        "task_queue", // queue
        "",           // consumer
        false,        // auto-ack
        false,        // exclusive
        false,        // no-local
        false,        // no-wait
        nil,          // args
    )
    if err != nil {
        return err
    }
    
    go func() {
        for msg := range msgs {
            var task Task
            if err := json.Unmarshal(msg.Body, &task); err != nil {
                msg.Nack(false, false) // 拒绝消息，不重新入队
                continue
            }
            
            if err := handler(&task); err != nil {
                msg.Nack(false, true) // 拒绝消息，重新入队
            } else {
                msg.Ack(false) // 确认消息
            }
        }
    }()
    
    return nil
}
```

#### 事件驱动
- 采用事件驱动架构，便于后续服务解耦
- 实现事件的发布和订阅机制

```go
// 事件驱动示例
type Event struct {
    ID        string                 `json:"id"`
    Type      string                 `json:"type"`
    Source    string                 `json:"source"`
    Data      map[string]interface{} `json:"data"`
    Timestamp time.Time              `json:"timestamp"`
}

type EventBus struct {
    subscribers map[string][]func(Event)
    mutex       sync.RWMutex
}

func (eb *EventBus) Subscribe(eventType string, handler func(Event)) {
    eb.mutex.Lock()
    defer eb.mutex.Unlock()
    
    if eb.subscribers == nil {
        eb.subscribers = make(map[string][]func(Event))
    }
    
    eb.subscribers[eventType] = append(eb.subscribers[eventType], handler)
}

func (eb *EventBus) Publish(event Event) {
    eb.mutex.RLock()
    defer eb.mutex.RUnlock()
    
    if handlers, exists := eb.subscribers[event.Type]; exists {
        for _, handler := range handlers {
            go handler(event) // 异步处理事件
        }
    }
}
```

## 5. 任务调度设计

### 5.1 分布式调度准备

#### 任务状态外部化
- 任务状态存储在数据库中，而非内存
- 支持任务状态的实时查询和更新

```go
// 任务状态管理
type TaskStatus string

const (
    TaskStatusPending   TaskStatus = "pending"
    TaskStatusRunning   TaskStatus = "running"
    TaskStatusCompleted TaskStatus = "completed"
    TaskStatusFailed    TaskStatus = "failed"
    TaskStatusCancelled TaskStatus = "cancelled"
)

type Task struct {
    ID          string                 `json:"id" db:"id"`
    UserID      string                 `json:"user_id" db:"user_id"`
    Type        string                 `json:"type" db:"type"`
    Status      TaskStatus             `json:"status" db:"status"`
    Priority    int                    `json:"priority" db:"priority"`
    Config      map[string]interface{} `json:"config" db:"config"`
    Result      map[string]interface{} `json:"result" db:"result"`
    AgentID     string                 `json:"agent_id" db:"agent_id"`
    CreatedAt   time.Time              `json:"created_at" db:"created_at"`
    UpdatedAt   time.Time              `json:"updated_at" db:"updated_at"`
    StartedAt   *time.Time             `json:"started_at" db:"started_at"`
    CompletedAt *time.Time             `json:"completed_at" db:"completed_at"`
}
```

#### 分布式锁
- 使用Redis或etcd实现分布式锁，避免任务重复调度
- 实现锁的自动续期和释放机制

```go
// 分布式锁示例
type DistributedLock struct {
    redis  *redis.Client
    key    string
    value  string
    expiry time.Duration
}

func NewDistributedLock(redis *redis.Client, key string, expiry time.Duration) *DistributedLock {
    return &DistributedLock{
        redis:  redis,
        key:    key,
        value:  uuid.New().String(),
        expiry: expiry,
    }
}

func (dl *DistributedLock) Acquire() (bool, error) {
    result, err := dl.redis.SetNX(dl.key, dl.value, dl.expiry).Result()
    if err != nil {
        return false, err
    }
    
    if result {
        // 启动自动续期
        go dl.autoRenew()
    }
    
    return result, nil
}

func (dl *DistributedLock) Release() error {
    script := `
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
    `
    _, err := dl.redis.Eval(script, []string{dl.key}, dl.value).Result()
    return err
}

func (dl *DistributedLock) autoRenew() {
    ticker := time.NewTicker(dl.expiry / 3)
    defer ticker.Stop()
    
    for range ticker.C {
        script := `
            if redis.call("get", KEYS[1]) == ARGV[1] then
                return redis.call("expire", KEYS[1], ARGV[2])
            else
                return 0
            end
        `
        result, err := dl.redis.Eval(script, []string{dl.key}, dl.value, int(dl.expiry.Seconds())).Result()
        if err != nil || result.(int64) == 0 {
            break // 锁已被释放或过期
        }
    }
}
```

#### 任务队列
- 使用外部消息队列管理任务，支持多消费者模式
- 实现任务的优先级和延迟执行

```go
// 任务队列管理
type TaskQueue struct {
    mq     *MessageQueue
    redis  *redis.Client
    db     *sql.DB
}

func (tq *TaskQueue) EnqueueTask(task *Task) error {
    // 1. 保存任务到数据库
    if err := tq.saveTaskToDB(task); err != nil {
        return err
    }
    
    // 2. 发送任务到消息队列
    if err := tq.mq.PublishTask(task); err != nil {
        // 如果消息队列发送失败，更新任务状态
        tq.updateTaskStatus(task.ID, TaskStatusFailed)
        return err
    }
    
    return nil
}

func (tq *TaskQueue) DequeueTask() (*Task, error) {
    // 从消息队列获取任务
    return tq.mq.ConsumeTask(func(task *Task) error {
        // 更新任务状态为运行中
        return tq.updateTaskStatus(task.ID, TaskStatusRunning)
    })
}
```

### 5.2 负载均衡算法

#### 插件化设计
- 负载均衡算法设计为可插拔的组件
- 支持多种负载均衡策略

```go
// 负载均衡接口
type LoadBalancer interface {
    SelectAgent(agents []*Agent, task *Task) (*Agent, error)
}

// 轮询负载均衡
type RoundRobinBalancer struct {
    counter int64
}

func (rb *RoundRobinBalancer) SelectAgent(agents []*Agent, task *Task) (*Agent, error) {
    if len(agents) == 0 {
        return nil, errors.New("no available agents")
    }
    
    index := atomic.AddInt64(&rb.counter, 1) % int64(len(agents))
    return agents[index], nil
}

// 最少连接负载均衡
type LeastConnectionsBalancer struct{}

func (lcb *LeastConnectionsBalancer) SelectAgent(agents []*Agent, task *Task) (*Agent, error) {
    if len(agents) == 0 {
        return nil, errors.New("no available agents")
    }
    
    var selectedAgent *Agent
    minConnections := int(^uint(0) >> 1) // 最大整数
    
    for _, agent := range agents {
        if agent.ActiveTasks < minConnections {
            minConnections = agent.ActiveTasks
            selectedAgent = agent
        }
    }
    
    return selectedAgent, nil
}

// 加权负载均衡
type WeightedBalancer struct{}

func (wb *WeightedBalancer) SelectAgent(agents []*Agent, task *Task) (*Agent, error) {
    if len(agents) == 0 {
        return nil, errors.New("no available agents")
    }
    
    totalWeight := 0
    for _, agent := range agents {
        totalWeight += agent.Weight
    }
    
    if totalWeight == 0 {
        return agents[0], nil
    }
    
    random := rand.Intn(totalWeight)
    currentWeight := 0
    
    for _, agent := range agents {
        currentWeight += agent.Weight
        if random < currentWeight {
            return agent, nil
        }
    }
    
    return agents[len(agents)-1], nil
}
```

#### Agent状态管理
- 实时收集和更新Agent状态信息
- 支持Agent的健康检查和故障检测

```go
// Agent状态
type Agent struct {
    ID           string    `json:"id"`
    Name         string    `json:"name"`
    Address      string    `json:"address"`
    Status       string    `json:"status"`
    Capabilities []string  `json:"capabilities"`
    Resource     Resource  `json:"resource"`
    ActiveTasks  int       `json:"active_tasks"`
    Weight       int       `json:"weight"`
    LastHeartbeat time.Time `json:"last_heartbeat"`
}

type Resource struct {
    CPUCores   int   `json:"cpu_cores"`
    MemoryMB   int64 `json:"memory_mb"`
    DiskGB     int64 `json:"disk_gb"`
    CPUUsage   float64 `json:"cpu_usage"`
    MemoryUsage float64 `json:"memory_usage"`
    DiskUsage   float64 `json:"disk_usage"`
}

// Agent管理器
type AgentManager struct {
    agents map[string]*Agent
    mutex  sync.RWMutex
    redis  *redis.Client
}

func (am *AgentManager) UpdateAgentStatus(agentID string, status *Agent) {
    am.mutex.Lock()
    defer am.mutex.Unlock()
    
    am.agents[agentID] = status
    
    // 同步到Redis
    data, _ := json.Marshal(status)
    am.redis.HSet("agents", agentID, data)
}

func (am *AgentManager) GetAvailableAgents() []*Agent {
    am.mutex.RLock()
    defer am.mutex.RUnlock()
    
    var available []*Agent
    for _, agent := range am.agents {
        if agent.Status == "online" && time.Since(agent.LastHeartbeat) < time.Minute*2 {
            available = append(available, agent)
        }
    }
    
    return available
}
```

#### 任务分片
- 大任务支持分片处理，便于分布式执行
- 实现任务的合并和结果聚合

```go
// 任务分片
type TaskShard struct {
    ID       string      `json:"id"`
    ParentID string      `json:"parent_id"`
    Index    int         `json:"index"`
    Total    int         `json:"total"`
    Data     interface{} `json:"data"`
}

type TaskSplitter interface {
    SplitTask(task *Task) ([]*TaskShard, error)
    MergeResults(shards []*TaskShard) (*Task, error)
}

// IP扫描任务分片器
type IPScanSplitter struct{}

func (iss *IPScanSplitter) SplitTask(task *Task) ([]*TaskShard, error) {
    targets := task.Config["targets"].([]string)
    shardSize := 100 // 每个分片100个目标
    
    var shards []*TaskShard
    for i := 0; i < len(targets); i += shardSize {
        end := i + shardSize
        if end > len(targets) {
            end = len(targets)
        }
        
        shard := &TaskShard{
            ID:       uuid.New().String(),
            ParentID: task.ID,
            Index:    len(shards),
            Total:    (len(targets) + shardSize - 1) / shardSize,
            Data:     targets[i:end],
        }
        
        shards = append(shards, shard)
    }
    
    return shards, nil
}

func (iss *IPScanSplitter) MergeResults(shards []*TaskShard) (*Task, error) {
    // 合并扫描结果
    var allResults []interface{}
    for _, shard := range shards {
        if results, ok := shard.Data.([]interface{}); ok {
            allResults = append(allResults, results...)
        }
    }
    
    task := &Task{
        ID:     shards[0].ParentID,
        Status: TaskStatusCompleted,
        Result: map[string]interface{}{
            "results": allResults,
            "total":   len(allResults),
        },
    }
    
    return task, nil
}
```

## 6. 监控和日志

### 6.1 可观测性

#### 结构化日志
- 使用结构化日志格式，便于集中收集和分析
- 统一日志级别和格式

```go
// 日志配置
type LogConfig struct {
    Level  string `json:"level"`
    Format string `json:"format"` // json 或 text
    Output string `json:"output"` // stdout, file, 或 syslog
}

// 结构化日志示例
func setupLogger(config LogConfig) *logrus.Logger {
    logger := logrus.New()
    
    // 设置日志级别
    level, err := logrus.ParseLevel(config.Level)
    if err != nil {
        level = logrus.InfoLevel
    }
    logger.SetLevel(level)
    
    // 设置日志格式
    if config.Format == "json" {
        logger.SetFormatter(&logrus.JSONFormatter{
            TimestampFormat: time.RFC3339,
            FieldMap: logrus.FieldMap{
                logrus.FieldKeyTime:  "timestamp",
                logrus.FieldKeyLevel: "level",
                logrus.FieldKeyMsg:   "message",
            },
        })
    } else {
        logger.SetFormatter(&logrus.TextFormatter{
            TimestampFormat: time.RFC3339,
            FullTimestamp:   true,
        })
    }
    
    return logger
}

// 使用示例
func (ts *TaskService) CreateTask(task *Task) error {
    logger := ts.logger.WithFields(logrus.Fields{
        "task_id":   task.ID,
        "task_type": task.Type,
        "user_id":   task.UserID,
        "trace_id":  getTraceID(), // 链路追踪ID
    })
    
    logger.Info("Creating new task")
    
    if err := ts.validateTask(task); err != nil {
        logger.WithError(err).Error("Task validation failed")
        return err
    }
    
    if err := ts.saveTask(task); err != nil {
        logger.WithError(err).Error("Failed to save task")
        return err
    }
    
    logger.Info("Task created successfully")
    return nil
}
```

#### 指标收集
- 集成Prometheus指标收集，为集群监控做准备
- 定义关键业务指标

```go
// Prometheus指标定义
var (
    taskCounter = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "neoscan_tasks_total",
            Help: "Total number of tasks",
        },
        []string{"type", "status"},
    )
    
    taskDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "neoscan_task_duration_seconds",
            Help:    "Task execution duration",
            Buckets: prometheus.DefBuckets,
        },
        []string{"type"},
    )
    
    agentCount = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "neoscan_agents_count",
            Help: "Number of agents by status",
        },
        []string{"status"},
    )
    
    httpRequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "neoscan_http_request_duration_seconds",
            Help:    "HTTP request duration",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint", "status"},
    )
)

func init() {
    prometheus.MustRegister(taskCounter)
    prometheus.MustRegister(taskDuration)
    prometheus.MustRegister(agentCount)
    prometheus.MustRegister(httpRequestDuration)
}

// 指标收集中间件
func PrometheusMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        start := time.Now()
        
        c.Next()
        
        duration := time.Since(start)
        status := strconv.Itoa(c.Writer.Status())
        
        httpRequestDuration.WithLabelValues(
            c.Request.Method,
            c.FullPath(),
            status,
        ).Observe(duration.Seconds())
    }
}

// 业务指标收集
func (ts *TaskService) recordTaskMetrics(task *Task, duration time.Duration) {
    taskCounter.WithLabelValues(task.Type, string(task.Status)).Inc()
    
    if task.Status == TaskStatusCompleted || task.Status == TaskStatusFailed {
        taskDuration.WithLabelValues(task.Type).Observe(duration.Seconds())
    }
}
```

#### 链路追踪
- 实现分布式链路追踪，便于问题定位
- 集成OpenTelemetry或Jaeger

```go
// 链路追踪配置
func setupTracing(serviceName string) (trace.TracerProvider, error) {
    exporter, err := jaeger.New(jaeger.WithCollectorEndpoint(
        jaeger.WithEndpoint("http://localhost:14268/api/traces"),
    ))
    if err != nil {
        return nil, err
    }
    
    tp := trace.NewTracerProvider(
        trace.WithBatcher(exporter),
        trace.WithResource(resource.NewWithAttributes(
            semconv.SchemaURL,
            semconv.ServiceNameKey.String(serviceName),
            semconv.ServiceVersionKey.String("1.0.0"),
        )),
    )
    
    otel.SetTracerProvider(tp)
    return tp, nil
}

// 链路追踪中间件
func TracingMiddleware() gin.HandlerFunc {
    return otelgin.Middleware("neoscan-master")
}

// 业务代码中使用链路追踪
func (ts *TaskService) CreateTask(ctx context.Context, task *Task) error {
    ctx, span := otel.Tracer("task-service").Start(ctx, "CreateTask")
    defer span.End()
    
    span.SetAttributes(
        attribute.String("task.id", task.ID),
        attribute.String("task.type", task.Type),
        attribute.String("user.id", task.UserID),
    )
    
    if err := ts.validateTask(ctx, task); err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, err.Error())
        return err
    }
    
    if err := ts.saveTask(ctx, task); err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, err.Error())
        return err
    }
    
    span.SetStatus(codes.Ok, "Task created successfully")
    return nil
}
```

### 6.2 健康检查

#### 多层次检查
- 实现应用层、数据库层、缓存层的健康检查
- 支持深度健康检查和快速健康检查

```go
// 健康检查接口
type HealthChecker interface {
    Name() string
    Check(ctx context.Context) error
}

// 数据库健康检查
type DatabaseHealthChecker struct {
    db *sql.DB
}

func (dhc *DatabaseHealthChecker) Name() string {
    return "database"
}

func (dhc *DatabaseHealthChecker) Check(ctx context.Context) error {
    ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
    defer cancel()
    
    return dhc.db.PingContext(ctx)
}

// Redis健康检查
type RedisHealthChecker struct {
    redis *redis.Client
}

func (rhc *RedisHealthChecker) Name() string {
    return "redis"
}

func (rhc *RedisHealthChecker) Check(ctx context.Context) error {
    ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
    defer cancel()
    
    return rhc.redis.Ping().Err()
}

// 健康检查管理器
type HealthManager struct {
    checkers []HealthChecker
}

func (hm *HealthManager) AddChecker(checker HealthChecker) {
    hm.checkers = append(hm.checkers, checker)
}

func (hm *HealthManager) CheckHealth(ctx context.Context) map[string]interface{} {
    result := make(map[string]interface{})
    overall := "healthy"
    
    for _, checker := range hm.checkers {
        start := time.Now()
        err := checker.Check(ctx)
        duration := time.Since(start)
        
        status := map[string]interface{}{
            "status":   "healthy",
            "duration": duration.String(),
        }
        
        if err != nil {
            status["status"] = "unhealthy"
            status["error"] = err.Error()
            overall = "unhealthy"
        }
        
        result[checker.Name()] = status
    }
    
    result["overall"] = overall
    result["timestamp"] = time.Now().Format(time.RFC3339)
    
    return result
}

// 健康检查端点
func (hm *HealthManager) HealthHandler(c *gin.Context) {
    ctx, cancel := context.WithTimeout(c.Request.Context(), 10*time.Second)
    defer cancel()
    
    health := hm.CheckHealth(ctx)
    
    if health["overall"] == "healthy" {
        c.JSON(200, health)
    } else {
        c.JSON(503, health)
    }
}

func (hm *HealthManager) ReadinessHandler(c *gin.Context) {
    // 简单的就绪检查，只检查关键依赖
    ctx, cancel := context.WithTimeout(c.Request.Context(), 5*time.Second)
    defer cancel()
    
    for _, checker := range hm.checkers {
        if checker.Name() == "database" || checker.Name() == "redis" {
            if err := checker.Check(ctx); err != nil {
                c.JSON(503, map[string]interface{}{
                    "status": "not ready",
                    "error":  err.Error(),
                })
                return
            }
        }
    }
    
    c.JSON(200, map[string]interface{}{
        "status": "ready",
    })
}
```

#### 优雅关闭
- 实现优雅关闭机制，支持滚动更新
- 确保正在处理的请求能够完成

```go
// 优雅关闭
type GracefulShutdown struct {
    server   *http.Server
    shutdown chan os.Signal
    done     chan bool
}

func NewGracefulShutdown(server *http.Server) *GracefulShutdown {
    return &GracefulShutdown{
        server:   server,
        shutdown: make(chan os.Signal, 1),
        done:     make(chan bool, 1),
    }
}

func (gs *GracefulShutdown) Start() {
    signal.Notify(gs.shutdown, os.Interrupt, syscall.SIGTERM)
    
    go func() {
        <-gs.shutdown
        
        log.Println("Server is shutting down...")
        
        ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
        defer cancel()
        
        // 停止接受新请求
        if err := gs.server.Shutdown(ctx); err != nil {
            log.Printf("Server forced to shutdown: %v", err)
        }
        
        log.Println("Server exited")
        gs.done <- true
    }()
}

func (gs *GracefulShutdown) Wait() {
    <-gs.done
}

// 使用示例
func main() {
    router := gin.Default()
    // 设置路由...
    
    server := &http.Server{
        Addr:    ":8080",
        Handler: router,
    }
    
    graceful := NewGracefulShutdown(server)
    graceful.Start()
    
    log.Println("Server starting on :8080")
    if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
        log.Fatalf("Server failed to start: %v", err)
    }
    
    graceful.Wait()
}
```

## 7. 部署和运维

### 7.1 容器化

#### Docker化
- 从开发阶段就使用Docker容器
- 多阶段构建优化镜像大小

```dockerfile
# Dockerfile
# 构建阶段
FROM golang:1.19-alpine AS builder

WORKDIR /app

# 复制依赖文件
COPY go.mod go.sum ./
RUN go mod download

# 复制源代码
COPY . .

# 构建应用
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main ./cmd/master

# 运行阶段
FROM alpine:latest

# 安装必要的包
RUN apk --no-cache add ca-certificates tzdata

WORKDIR /root/

# 从构建阶段复制二进制文件
COPY --from=builder /app/main .

# 复制配置文件
COPY --from=builder /app/configs ./configs

# 暴露端口
EXPOSE 8080 9090

# 健康检查
HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# 运行应用
CMD ["./main"]
```

#### 配置分离
- 配置文件与镜像分离，支持不同环境部署
- 使用环境变量和配置文件结合的方式

```yaml
# docker-compose.yml
version: '3.8'

services:
  neoscan-master:
    build: .
    ports:
      - "8080:8080"
      - "9090:9090"
    environment:
      - GIN_MODE=release
      - DB_HOST=mysql
      - DB_PORT=3306
      - DB_USERNAME=neoscan
      - DB_PASSWORD=password
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./configs:/root/configs:ro
      - ./logs:/root/logs
    depends_on:
      - mysql
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  mysql:
    image: mysql:8.0
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=neoscan
      - MYSQL_USER=neoscan
      - MYSQL_PASSWORD=password
    volumes:
      - mysql_data:/var/lib/mysql
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "3306:3306"
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped

volumes:
  mysql_data:
  redis_data:
```

#### 资源限制
- 设置合理的资源限制，便于Kubernetes调度
- 配置内存和CPU限制

```yaml
# docker-compose.override.yml (生产环境)
version: '3.8'

services:
  neoscan-master:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
```

### 7.2 CI/CD准备

#### 自动化测试
- 完善的单元测试和集成测试
- 测试覆盖率要求

```go
// 单元测试示例
func TestTaskService_CreateTask(t *testing.T) {
    // 设置测试数据库
    db := setupTestDB(t)
    defer db.Close()
    
    // 设置测试Redis
    redis := setupTestRedis(t)
    defer redis.Close()
    
    // 创建服务实例
    taskService := NewTaskService(db, redis)
    
    tests := []struct {
        name    string
        task    *Task
        wantErr bool
    }{
        {
            name: "valid task",
            task: &Task{
                ID:     "test-task-1",
                UserID: "user-1",
                Type:   "port_scan",
                Config: map[string]interface{}{
                    "targets": []string{"192.168.1.1"},
                },
            },
            wantErr: false,
        },
        {
            name: "invalid task - missing targets",
            task: &Task{
                ID:     "test-task-2",
                UserID: "user-1",
                Type:   "port_scan",
                Config: map[string]interface{}{},
            },
            wantErr: true,
        },
    }
    
    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            err := taskService.CreateTask(tt.task)
            if (err != nil) != tt.wantErr {
                t.Errorf("CreateTask() error = %v, wantErr %v", err, tt.wantErr)
            }
        })
    }
}

// 集成测试示例
func TestTaskAPI_Integration(t *testing.T) {
    // 设置测试环境
    testEnv := setupTestEnvironment(t)
    defer testEnv.Cleanup()
    
    // 创建测试客户端
    client := testEnv.Client()
    
    // 测试创建任务
    task := &Task{
        Type:   "port_scan",
        Config: map[string]interface{}{
            "targets": []string{"192.168.1.1"},
        },
    }
    
    resp, err := client.CreateTask(task)
    require.NoError(t, err)
    require.Equal(t, 201, resp.StatusCode)
    
    var createdTask Task
    err = json.NewDecoder(resp.Body).Decode(&createdTask)
    require.NoError(t, err)
    require.NotEmpty(t, createdTask.ID)
    
    // 测试获取任务
    resp, err = client.GetTask(createdTask.ID)
    require.NoError(t, err)
    require.Equal(t, 200, resp.StatusCode)
}
```

#### 蓝绿部署
- 支持蓝绿部署或滚动更新
- 实现部署脚本

```bash
#!/bin/bash
# deploy.sh - 蓝绿部署脚本

set -e

IMAGE_TAG=${1:-latest}
ENVIRONMENT=${2:-staging}

echo "Deploying NeoScan Master $IMAGE_TAG to $ENVIRONMENT"

# 构建新镜像
docker build -t neoscan/master:$IMAGE_TAG .

# 推送到镜像仓库
docker push neoscan/master:$IMAGE_TAG

# 更新docker-compose配置
sed -i "s/image: neoscan\/master:.*/image: neoscan\/master:$IMAGE_TAG/g" docker-compose.$ENVIRONMENT.yml

# 执行蓝绿部署
docker-compose -f docker-compose.$ENVIRONMENT.yml up -d --no-deps neoscan-master

# 健康检查
echo "Waiting for service to be healthy..."
for i in {1..30}; do
    if curl -f http://localhost:8080/health > /dev/null 2>&1; then
        echo "Service is healthy"
        break
    fi
    echo "Waiting... ($i/30)"
    sleep 10
done

# 清理旧镜像
docker image prune -f

echo "Deployment completed successfully"
```

#### 回滚机制
- 实现快速回滚到上一个版本
- 保留多个版本的镜像

```bash
#!/bin/bash
# rollback.sh - 回滚脚本

set -e

PREVIOUS_TAG=${1}
ENVIRONMENT=${2:-staging}

if [ -z "$PREVIOUS_TAG" ]; then
    echo "Usage: $0 <previous_tag> [environment]"
    exit 1
fi

echo "Rolling back to $PREVIOUS_TAG in $ENVIRONMENT"

# 更新配置
sed -i "s/image: neoscan\/master:.*/image: neoscan\/master:$PREVIOUS_TAG/g" docker-compose.$ENVIRONMENT.yml

# 执行回滚
docker-compose -f docker-compose.$ENVIRONMENT.yml up -d --no-deps neoscan-master

# 健康检查
echo "Waiting for service to be healthy..."
for i in {1..30}; do
    if curl -f http://localhost:8080/health > /dev/null 2>&1; then
        echo "Rollback completed successfully"
        exit 0
    fi
    echo "Waiting... ($i/30)"
    sleep 10
done

echo "Rollback failed - service is not healthy"
exit 1
```

## 8. 开发建议

### 8.1 推荐的代码组织结构

```
neoscan/
├── cmd/
│   └── master/
│       └── main.go                 # 应用入口
├── internal/
│   ├── config/
│   │   ├── config.go              # 配置管理
│   │   └── watcher.go             # 配置监听
│   ├── service/
│   │   ├── task.go                # 任务服务
│   │   ├── agent.go               # Agent管理服务
│   │   ├── user.go                # 用户服务
│   │   └── auth.go                # 认证服务
│   ├── repository/
│   │   ├── task.go                # 任务数据访问
│   │   ├── agent.go               # Agent数据访问
│   │   └── user.go                # 用户数据访问
│   ├── handler/
│   │   ├── task.go                # 任务API处理器
│   │   ├── agent.go               # Agent API处理器
│   │   └── health.go              # 健康检查处理器
│   ├── middleware/
│   │   ├── auth.go                # 认证中间件
│   │   ├── logging.go             # 日志中间件
│   │   ├── metrics.go             # 指标中间件
│   │   └── tracing.go             # 链路追踪中间件
│   ├── model/
│   │   ├── task.go                # 任务模型
│   │   ├── agent.go               # Agent模型
│   │   └── user.go                # 用户模型
│   ├── scheduler/
│   │   ├── scheduler.go           # 任务调度器
│   │   ├── balancer.go            # 负载均衡器
│   │   └── queue.go               # 任务队列
│   ├── registry/
│   │   ├── service.go             # 服务注册
│   │   └── discovery.go           # 服务发现
│   └── util/
│       ├── lock.go                # 分布式锁
│       ├── cache.go               # 缓存工具
│       └── crypto.go              # 加密工具
├── pkg/
│   ├── database/
│   │   ├── mysql.go               # MySQL连接管理
│   │   └── redis.go               # Redis连接管理
│   ├── queue/
│   │   └── rabbitmq.go            # RabbitMQ管理
│   ├── grpc/
│   │   └── client.go              # gRPC客户端
│   └── logger/
│       └── logger.go              # 日志工具
├── api/
│   └── proto/
│       ├── agent.proto            # Agent服务定义
│       └── task.proto             # 任务服务定义
├── configs/
│   ├── config.yaml                # 默认配置
│   ├── config.dev.yaml            # 开发环境配置
│   └── config.prod.yaml           # 生产环境配置
├── scripts/
│   ├── init.sql                   # 数据库初始化脚本
│   ├── deploy.sh                  # 部署脚本
│   └── rollback.sh                # 回滚脚本
├── docker-compose.yml             # 开发环境编排
├── docker-compose.prod.yml        # 生产环境编排
├── Dockerfile                     # Docker镜像构建
├── go.mod                         # Go模块定义
└── README.md                      # 项目说明
```

### 8.2 关键接口设计

#### 分布式调度接口

```go
// 分布式调度器接口
type DistributedScheduler interface {
    // 调度任务到可用的Agent
    ScheduleTask(ctx context.Context, task *Task) (*Agent, error)
    
    // 取消任务调度
    CancelTask(ctx context.Context, taskID string) error
    
    // 获取调度统计信息
    GetScheduleStats(ctx context.Context) (*ScheduleStats, error)
    
    // 重新平衡任务分配
    Rebalance(ctx context.Context) error
}

// 调度统计信息
type ScheduleStats struct {
    TotalTasks      int64             `json:"total_tasks"`
    PendingTasks    int64             `json:"pending_tasks"`
    RunningTasks    int64             `json:"running_tasks"`
    CompletedTasks  int64             `json:"completed_tasks"`
    FailedTasks     int64             `json:"failed_tasks"`
    AgentStats      map[string]*AgentStat `json:"agent_stats"`
}

type AgentStat struct {
    AgentID      string  `json:"agent_id"`
    ActiveTasks  int     `json:"active_tasks"`
    CPUUsage     float64 `json:"cpu_usage"`
    MemoryUsage  float64 `json:"memory_usage"`
    LastHeartbeat time.Time `json:"last_heartbeat"`
}
```

#### 配置管理接口

```go
// 配置管理器接口
type ConfigManager interface {
    // 获取配置值
    Get(ctx context.Context, key string) (interface{}, error)
    
    // 设置配置值
    Set(ctx context.Context, key string, value interface{}) error
    
    // 监听配置变化
    Watch(ctx context.Context, key string) (<-chan ConfigEvent, error)
    
    // 获取所有配置
    GetAll(ctx context.Context) (map[string]interface{}, error)
    
    // 批量设置配置
    SetBatch(ctx context.Context, configs map[string]interface{}) error
}

// 配置变化事件
type ConfigEvent struct {
    Key       string      `json:"key"`
    OldValue  interface{} `json:"old_value"`
    NewValue  interface{} `json:"new_value"`
    EventType string      `json:"event_type"` // create, update, delete
    Timestamp time.Time   `json:"timestamp"`
}
```

#### 服务发现接口

```go
// 服务发现接口
type ServiceDiscovery interface {
    // 注册服务
    Register(ctx context.Context, service *ServiceInfo) error
    
    // 注销服务
    Deregister(ctx context.Context, serviceID string) error
    
    // 发现服务
    Discover(ctx context.Context, serviceName string) ([]*ServiceInfo, error)
    
    // 监听服务变化
    Watch(ctx context.Context, serviceName string) (<-chan ServiceEvent, error)
    
    // 健康检查
    HealthCheck(ctx context.Context, serviceID string) error
}

// 服务信息
type ServiceInfo struct {
    ID       string            `json:"id"`
    Name     string            `json:"name"`
    Address  string            `json:"address"`
    Port     int               `json:"port"`
    Tags     []string          `json:"tags"`
    Metadata map[string]string `json:"metadata"`
    Health   string            `json:"health"`
}

// 服务变化事件
type ServiceEvent struct {
    Type    string       `json:"type"` // register, deregister, health_change
    Service *ServiceInfo `json:"service"`
}
```

#### 负载均衡接口

```go
// 负载均衡器接口
type LoadBalancer interface {
    // 选择服务实例
    Select(ctx context.Context, services []*ServiceInfo, request *Request) (*ServiceInfo, error)
    
    // 更新服务权重
    UpdateWeight(ctx context.Context, serviceID string, weight int) error
    
    // 获取负载均衡统计
    GetStats(ctx context.Context) (*LoadBalanceStats, error)
    
    // 设置负载均衡算法
    SetAlgorithm(algorithm string) error
}

// 负载均衡统计
type LoadBalanceStats struct {
    Algorithm     string                    `json:"algorithm"`
    TotalRequests int64                     `json:"total_requests"`
    ServiceStats  map[string]*ServiceStats  `json:"service_stats"`
}

type ServiceStats struct {
    ServiceID     string  `json:"service_id"`
    RequestCount  int64   `json:"request_count"`
    SuccessCount  int64   `json:"success_count"`
    ErrorCount    int64   `json:"error_count"`
    AvgResponseTime float64 `json:"avg_response_time"`
    Weight        int     `json:"weight"`
}
```

## 9. 总结

通过以上设计和实现建议，可以确保NeoScan项目在开发阶段就具备了向Master集群架构平滑过渡的能力。关键要点包括：

1. **架构设计**：采用微服务架构和无状态设计，为集群化做好准备
2. **状态管理**：外部化所有状态信息，避免本地缓存依赖
3. **配置管理**：使用外部配置中心，支持动态配置和服务发现
4. **数据存储**：设计读写分离和分布式缓存策略
5. **通信协议**：标准化API设计，支持负载均衡和故障转移
6. **任务调度**：分布式调度设计，支持任务分片和负载均衡
7. **监控日志**：完善的可观测性设计，支持分布式链路追踪
8. **部署运维**：容器化部署，支持CI/CD和蓝绿部署

这些设计原则和实现方案将确保项目能够从单Master开发环境无缝过渡到生产环境的Master集群架构，最大化代码复用，最小化重构成本。